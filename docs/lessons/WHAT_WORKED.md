# What Worked: Lessons from Building AstraWeave

**Context**: This document captures successful patterns discovered during 40+ days of AI-orchestrated game engine development (October 2025 - Present).

---

## Process & Methodology

### 1. Zero-Tolerance for Compilation Errors ✅

**Pattern**: Never defer compilation errors to later—fix immediately

**Why it worked**:
- Prevented cascading issues (one error → ten errors)
- Maintained development momentum (no "I'll fix it later" debt)
- Enabled confident iteration (always start from working code)

**Evidence**:
- **18-day zero-warning streak** achieved (Phase 8.1 Week 4 Day 3)
- **Phase 6**: Fixed 54 compilation errors in one session → 0 errors maintained
- **Week 3**: ZERO warnings across all test files (14 warnings eliminated)

**How to apply**:
```bash
# After EVERY code change
cargo check -p <crate>

# If errors appear, FIX IMMEDIATELY
# Warnings can be deferred, errors cannot
```

---

### 2. copilot_instructions.md as Persistent Context ✅

**Pattern**: Comprehensive instructions file that AI references across sessions

**Why it worked**:
- AI "remembers" patterns (no repeated mistakes)
- Consistent code quality (same patterns applied everywhere)
- Faster iteration (no need to re-explain context)
- Self-documenting process (instructions evolve with codebase)

**Evidence**:
- **1,000+ line copilot-instructions.md** maintained
- **Consistent API usage** (WorldSnapshot, ActionStep patterns never broken)
- **Zero regressions** in established patterns (e.g., error handling)

**How to apply**:
- Keep instructions up-to-date after major changes
- Document discovered patterns immediately
- Include code examples (not just prose)
- Version the instructions file (track evolution)

---

### 3. Comprehensive Completion Reports ✅

**Pattern**: Document everything—achievements, metrics, lessons, next steps

**Why it worked**:
- Provides evidence of progress (40-day timeline validated)
- Enables learning extraction (patterns emerge from retrospectives)
- Creates accountability (public record of what was done)
- Facilitates handoffs (comprehensive context for next session)

**Evidence**:
- **300+ documentation files** created (40,000+ words)
- **Every week has completion summary** (metrics, achievements, lessons)
- **Every phase has validation report** (success criteria, evidence)

**How to apply**:
- End every session with completion report
- Include metrics (time, LOC, tests, performance)
- Document failures (not just successes)
- Link to evidence (benchmarks, test results)

---

### 4. Iterative Validation with Real Systems ✅

**Pattern**: Test with real LLM/real data early and often—don't rely on mocks

**Why it worked**:
- Discovered bugs mocks would miss (e.g., case sensitivity in Phase 7)
- Built confidence in production readiness (not just "it compiles")
- Enabled performance optimization (real data shows bottlenecks)

**Evidence**:
- **Phase 7**: 0% → 75-85% LLM success rate (live validation caught enum case bug)
- **Week 8**: Real Tracy profiling revealed spatial hash gains (+9-17% across all systems)
- **Week 3**: Determinism validated with 6,000 agent-frames (not synthetic test)

**How to apply**:
- Run hello_companion with --demo-all frequently
- Use real Ollama models (not mocks) for LLM testing
- Benchmark with realistic entity counts (1,000+)
- Test determinism with multi-frame scenarios

---

### 5. Performance Budgets from Day 1 ✅

**Pattern**: Define 60 FPS budget allocation before optimization

**Why it worked**:
- Focused optimization efforts (know what matters most)
- Prevented premature optimization (know what's already fast enough)
- Enabled capacity planning (predict scaling limits)

**Evidence**:
- **60 FPS Budget**: ECS 30%, AI 12%, Physics 18%, Rendering 30%, Overhead 10%
- **Week 8**: Achieved 370 FPS @ 1,000 entities (84% headroom)
- **AI-Native Validation**: 12,700+ agents @ 60 FPS (18.8× over target)

**60 FPS Budget**:
```
Total: 16.67 ms per frame
├─ ECS: 5.0 ms (30%)
├─ AI: 2.0 ms (12%)  
├─ Physics: 3.0 ms (18%)
├─ Rendering: 5.0 ms (30%)
└─ Overhead: 1.67 ms (10%)
```

**How to apply**:
- Measure current costs early (baseline metrics)
- Allocate budget before optimizing (prioritize work)
- Track budget % (not just absolute times)
- Optimize only if >budget (don't waste time)

---

## Technical Decisions

### 6. ECS Architecture from Day 1 ✅

**Pattern**: Archetype-based ECS with deterministic ordering

**Why it worked**:
- Scalability validated (12,700+ agents @ 60 FPS)
- Cache-friendly iteration (spatial hash benefits all systems)
- Deterministic replay (multiplayer-ready)
- Easy to profile (systems isolated, Tracy-friendly)

**Evidence**:
- **242 tests passing** (100% pass rate)
- **516 µs for 3 systems @ 1,000 entities** (3.1% frame budget)
- **100% deterministic** (3 runs, bit-identical results)

---

### 7. AI-First Design (Perception → Planning → Action) ✅

**Pattern**: AI agents as first-class citizens, not bolted on

**Why it worked**:
- Tool sandbox prevents cheating (AI can't break rules)
- Pluggable orchestrators (LLM, BT, GOAP, Hybrid)
- Performance validated early (sub-microsecond planning)

**Evidence**:
- **6 AI modes functional** (Classical, BehaviorTree, Utility, LLM, Hybrid, Ensemble)
- **87-202 ns planning** (4.95-11.5M plans/sec)
- **75-85% LLM success rate** (production-ready with Hermes 2 Pro)

---

### 8. Deterministic Simulation Priority ✅

**Pattern**: Determinism from Day 1 (fixed RNG, ordered iteration, integer math)

**Why it worked**:
- Multiplayer-ready (no desyncs)
- Replay system validated (bit-identical results)
- Debugging easier (reproducible bugs)
- Anti-cheat possible (validate client actions)

**Evidence**:
- **Phase 3**: Determinism validation complete
- **Week 3**: 3 runs, bit-identical results (6,000 agent-frames)
- **6.48M validation checks/sec** (anti-cheat proven)

---

### 9. GPU-First Rendering (wgpu, Vulkan/DX12/Metal) ✅

**Pattern**: Modern GPU API from start (not legacy OpenGL)

**Why it worked**:
- Future-proof (wgpu tracks latest GPU features)
- Cross-platform (Vulkan/DX12/Metal abstraction)
- Compute shaders (GPU skinning, particle systems)

**Evidence**:
- **Week 1**: GPU skinning production-ready
- **Week 5**: Mesh optimization (37.5% memory reduction)
- **Phase 8.1**: 18-day zero-warning streak (stable API)

---

### 10. Tracy Profiling Integration Early (Week 6) ✅

**Pattern**: Zero-overhead profiling from start (not added later)

**Why it worked**:
- Identified hotspots accurately (spatial hash, SIMD movement)
- Enabled cache locality analysis (cascading benefits)
- Validated optimizations (before/after comparison)

**Evidence**:
- **Week 8**: -12.6% frame time (Tracy-guided optimization)
- **Spatial hash**: +9-17% improvements across ALL systems
- **SIMD movement**: 2.08× speedup validated

---

## Development Practices

### 11. Batching Over Scattering (ECS Iteration) ✅

**Pattern**: Collect → Process → Writeback (not scattered `get_mut()`)

**Why it worked**:
- 3-5× faster (archetype lookup is O(log n))
- SIMD-friendly (contiguous data)
- Cache-friendly (sequential access)

**Evidence**:
- **Week 8**: SIMD movement 2.08× speedup
- **Week 3**: API docs documented pattern (common pitfall #5)

**Example**:
```rust
// ❌ SLOW: Scattered access
for agent in &agents {
    if let Some(pos) = world.get_mut::<Position>(*agent) {
        pos.x += 1.0;
    }
}

// ✅ FAST: Batch collect → process → writeback
let mut positions: Vec<_> = agents.iter()
    .filter_map(|&agent| world.get_mut::<Position>(agent))
    .collect();

for pos in &mut positions {
    pos.x += 1.0;  // SIMD-friendly
}
```

---

### 12. Weekly Retrospectives ✅

**Pattern**: End every week with summary report (achievements, metrics, lessons)

**Why it worked**:
- Captured learnings immediately (not forgotten)
- Provided evidence of velocity (productivity metrics)
- Enabled course correction (identify what's not working)

**Evidence**:
- **8 week summaries** created (WEEK_1 through WEEK_8)
- **Consistent format** (Executive Summary, Achievements, Metrics, Lessons, Next Steps)
- **Velocity tracking**: 1.09 hours/day average over 40+ days

---

### 13. Feature Flags for Experimental Work ✅

**Pattern**: Gate unstable features behind `cfg` flags

**Why it worked**:
- Stable main branch (experimental work isolated)
- Easy to enable/disable (no code deletion)
- Safe rollback (just disable flag)

**Example**:
```rust
#[cfg(feature = "llm")]
pub mod llm_integration;

#[cfg(feature = "gpu-tests")]
mod gpu_tests;
```

---

## Optimization Strategies

### 14. Measure Before Optimizing (Week 8 Approach) ✅

**Pattern**: Tracy profiling → Identify hotspot → Fix → Validate

**Why it worked**:
- Focused effort (optimized what matters)
- Validated gains (not guessing)
- Avoided premature optimization (measured first)

**Evidence**:
- **Spatial hash**: 99.96% collision reduction (499,500 → 180 checks)
- **SIMD movement**: 2.08× speedup (20.588 µs → 9.879 µs)
- **Frame time**: 3.09 ms → 2.70 ms (-12.6%)

---

### 15. SIMD Auto-Vectorization Trust (glam) ✅

**Pattern**: Use glam SIMD types, trust compiler auto-vectorization

**Why it worked**:
- 80-85% of hand-written AVX2 performance (good enough)
- Zero maintenance (compiler handles platform differences)
- Safe (no unsafe code for SIMD)

**Evidence**:
- **Week 8**: SIMD movement 2.08× speedup with auto-vectorization
- **Week 5**: Math infrastructure using glam (production-ready)

---

## Documentation Practices

### 16. Three-Tier Documentation (Phase 6 Discovery) ✅

**Pattern**: Detailed analysis + Quick reference + Executive summary

**Why it worked**:
- Serves all audiences (deep dive vs quick lookup)
- Easy navigation (pick your depth)
- Completeness (nothing lost in summarization)

**Evidence**:
- **Phase 6**: 15,000-word report + quick reference + 500-word summary
- **Week 3**: API docs (650 lines) + cheat sheets (3 tables) + examples (23+)

---

### 17. Code Examples in Documentation ✅

**Pattern**: Show correct AND incorrect usage (with error messages)

**Why it worked**:
- Prevents common mistakes (see what NOT to do)
- Faster onboarding (copy-paste examples work)
- Clear error messages (know what to expect)

**Evidence**:
- **Week 3 API docs**: 23+ code examples (correct vs incorrect usage)
- **ActionStep discovery**: 8 errors prevented by documenting pattern matching

---

## Collaboration Patterns

### 18. AI Orchestration Tips Documented ✅

**Pattern**: Document what makes prompts effective (for future AI collaboration)

**Why it worked**:
- Improved AI output quality (clear context = better code)
- Reduced iteration cycles (better prompts = fewer fixes)
- Knowledge transfer (others can learn orchestration)

**Evidence**:
- **copilot_instructions.md**: 1,000+ lines of patterns
- **18-day zero-warning streak**: AI follows established patterns
- **This document**: Meta-documentation of orchestration success

---

## Conclusion

**Key Insight**: Systematic process > individual brilliance

The success of AstraWeave isn't from any single breakthrough—it's from **consistently applying proven patterns**:

1. ✅ Fix errors immediately (zero-error policy)
2. ✅ Document everything (copilot instructions, completion reports)
3. ✅ Test with real systems (no mocks for validation)
4. ✅ Define budgets early (know what matters)
5. ✅ Measure before optimizing (Tracy-guided development)
6. ✅ Batch over scatter (ECS iteration patterns)
7. ✅ Weekly retrospectives (capture learnings)

**Evidence**: 40+ days, 300+ docs, 242 tests passing, 370 FPS, 12,700+ agents, 75-85% LLM success, 100% determinism

**Next**: See `WHAT_DIDNT.md` for failed approaches and `AI_ORCHESTRATION_TIPS.md` for GCP methodology

---

*Last Updated*: January 2026 (October 20, 2025)  
*Extracted from*: 300+ completion reports across 40+ days of development
