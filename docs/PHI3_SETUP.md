# Phi-3 Setup Guide for AstraWeave

**AI-Powered Game Planning with Microsoft Phi-3 Medium**

This guide shows you how to enable real LLM-based AI agents in AstraWeave using Phi-3 Medium Q4 via Ollama.

---

## üöÄ Quick Start (5 Minutes)

### Step 1: Install Ollama

**Windows**:
```powershell
# Download from https://ollama.ai/download/windows
# Or use winget:
winget install Ollama.Ollama
```

**macOS**:
```bash
brew install ollama
```

**Linux**:
```bash
curl https://ollama.ai/install.sh | sh
```

### Step 2: Download Phi-3 Model

**‚ö†Ô∏è IMPORTANT: Choose the right model for your GPU!**

Check your VRAM:
```powershell
nvidia-smi --query-gpu=name,memory.total --format=csv
```

**If you have 4-8GB VRAM** (GTX 1650, 1660, RTX 2060):
```bash
ollama pull phi3:3.8b
ollama create phi3:game -f Modelfile.phi3-game  # See repo for modelfile
```
Expected latency: **2-6 seconds** ‚úÖ

**If you have 12GB+ VRAM** (RTX 3060, 4060, 3080):
```bash
ollama pull phi3:medium
```
Expected latency: **1-3 seconds** ‚úÖ

**If you have 20GB+ VRAM** (RTX 4090, A6000):
```bash
ollama pull phi3:large
```
Expected latency: **0.5-2 seconds** ‚úÖ

> **Note**: Using phi3:medium on <8GB VRAM will cause CPU fallback (60-140s latency!)
> See `PHI3_OPTIMIZATION_COMPLETE.md` for details.

### Step 3: Start Ollama Server

```bash
ollama serve
```

Leave this running in a terminal. Ollama starts on `http://localhost:11434`.

**Windows note**: Ollama may start automatically as a service after installation.

### Step 4: Test in AstraWeave

```bash
cd AstraWeave-AI-Native-Gaming-Engine

# Build with Ollama support
cargo build -p astraweave-llm --features ollama

# Run hello_companion example with Phi-3
cargo run -p hello_companion --features ollama --release
```

You should see AI plans generated by Phi-3 instead of the mock LLM!

---

## üìä Verify Setup

### Health Check Script

Create `test_phi3.rs`:
```rust
use astraweave_llm::phi3_ollama::Phi3Ollama;

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let client = Phi3Ollama::localhost();
    
    println!("üîç Checking Phi-3 setup...\n");
    
    let health = client.health_check().await?;
    
    if health.is_ready() {
        println!("‚úÖ Ollama server: Running");
        println!("‚úÖ Model phi3:medium: Available");
        println!("üì¶ Ollama version: {}", health.ollama_version);
        println!("\nüéâ Setup complete! Phi-3 is ready.");
    } else {
        eprintln!("‚ùå Setup incomplete:");
        eprintln!("{}", health.error_message().unwrap());
    }
    
    Ok(())
}
```

Run:
```bash
cargo run --bin test_phi3 --features ollama
```

### Test Inference

```rust
use astraweave_llm::phi3_ollama::Phi3Ollama;
use astraweave_llm::LlmClient;

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let client = Phi3Ollama::localhost();
    
    let prompt = "Enemy at (10,5). You are at (3,3). Generate a tactical plan in JSON.";
    let response = client.complete(prompt).await?;
    
    println!("Phi-3 Response:\n{}", response);
    Ok(())
}
```

Expected output (similar to):
```json
{
  "plan_id": "tactical-001",
  "reasoning": "Move to cover position, then engage with suppressive fire",
  "steps": [
    {"act": "MoveTo", "x": 5, "y": 4},
    {"act": "CoverFire", "target_id": 99, "duration": 3.0}
  ]
}
```

---

## ‚öôÔ∏è Configuration

### Temperature (Creativity vs. Determinism)

```rust
let client = Phi3Ollama::localhost()
    .with_temperature(0.3);  // Very deterministic (good for tactics)
    
let client = Phi3Ollama::localhost()
    .with_temperature(0.7);  // Balanced (default)
    
let client = Phi3Ollama::localhost()
    .with_temperature(1.0);  // Creative (good for dynamic situations)
```

**Recommendation**: 0.5-0.7 for game AI (balance between consistency and variety)

### Max Tokens (Plan Length)

```rust
let client = Phi3Ollama::localhost()
    .with_max_tokens(128);   // Short plans (1-3 actions)
    
let client = Phi3Ollama::localhost()
    .with_max_tokens(512);   // Default (5-10 actions)
    
let client = Phi3Ollama::localhost()
    .with_max_tokens(1024);  // Long-term plans (10+ actions)
```

**Recommendation**: 256-512 for most game scenarios

### Custom System Prompt

```rust
let client = Phi3Ollama::localhost()
    .with_system_prompt(
        "You are a stealth operative. Never use loud weapons. \
         Move silently and use distractions. Output valid JSON only."
    );
```

See `astraweave-llm/src/prompts.rs` for pre-built templates (TACTICAL, STEALTH, SUPPORT, EXPLORATION).

---

## üéÆ Using Phi-3 in Your Game

### Replace MockLlm

**Before** (mock testing):
```rust
use astraweave_llm::MockLlm;

let client = Arc::new(MockLlm);
```

**After** (real AI):
```rust
use astraweave_llm::phi3_ollama::Phi3Ollama;

let client = Arc::new(Phi3Ollama::localhost());
```

Same `LlmClient` trait, drop-in replacement!

### With Prompt Engineering

```rust
use astraweave_llm::phi3_ollama::Phi3Ollama;
use astraweave_llm::prompts::quick;
use astraweave_core::WorldSnapshot;

async fn plan_action(snapshot: &WorldSnapshot) -> anyhow::Result<String> {
    let client = Phi3Ollama::localhost();
    
    // Use tactical template with world state
    let prompt = quick::tactical_prompt(snapshot, "Eliminate all enemies");
    
    let response = client.complete(&prompt).await?;
    
    // Parse JSON response
    let plan: PlanIntent = serde_json::from_str(&response)?;
    
    Ok(plan.plan_id)
}
```

### With LlmScheduler (Non-blocking)

```rust
use astraweave_llm::scheduler::{LlmScheduler, RequestPriority};
use astraweave_llm::phi3_ollama::Phi3Ollama;

// In your game initialization:
let phi3 = Arc::new(Phi3Ollama::localhost());
let scheduler = LlmScheduler::new(phi3, 5, 30);  // 5 concurrent, 30s timeout

// In your AI tick system:
fn ai_system(scheduler: &LlmScheduler, snapshot: &WorldSnapshot) {
    // Submit non-blocking request
    let request_id = scheduler.submit_request(
        build_prompt(snapshot),
        RequestPriority::High
    ).await;
    
    // Poll later (don't block game loop!)
    if let Some(result) = scheduler.poll_result(request_id) {
        match result {
            RequestResult::Success(response) => {
                // Got AI plan!
                let plan = parse_llm_plan(&response, &registry)?;
                execute_plan(plan);
            }
            RequestResult::Failure(err) => {
                // Fallback to rule-based AI
                execute_fallback_behavior();
            }
        }
    }
}
```

---

## üêõ Troubleshooting

### "Ollama API returned error status: 404"

**Cause**: Ollama server not running or model not downloaded

**Fix**:
1. Check Ollama is running: `curl http://localhost:11434/api/tags`
2. If not, run: `ollama serve`
3. Verify model: `ollama list` (should show `phi3:medium`)
4. If missing: `ollama pull phi3:medium`

### "Failed to connect to Ollama server"

**Cause**: Ollama not installed or wrong URL

**Fix**:
1. Install Ollama: https://ollama.ai/download
2. Check service status:
   - Windows: Task Manager ‚Üí Services ‚Üí "Ollama"
   - macOS/Linux: `ps aux | grep ollama`
3. Restart: `ollama serve`

### Slow Inference (>5 seconds)

**Cause**: Running on CPU instead of GPU

**Fix**:
1. Check GPU availability: `ollama ps` (should show GPU device)
2. Update GPU drivers:
   - NVIDIA: https://www.nvidia.com/Download/index.aspx
   - AMD: https://www.amd.com/en/support
3. Verify CUDA/ROCm installed (if NVIDIA/AMD GPU)
4. Consider using `phi3:mini` (2.3GB, faster on CPU)

### Out of Memory (OOM)

**Cause**: Insufficient VRAM for phi3:medium

**Fix**:
1. Close other GPU-heavy applications
2. Use smaller model: `ollama pull phi3:mini`
3. Update client: `.with_model("phi3:mini")`
4. Or upgrade to 12GB+ VRAM GPU

### JSON Parsing Failures

**Cause**: Phi-3 generated invalid JSON

**Fix**:
1. Lower temperature: `.with_temperature(0.3)` (more deterministic)
2. Add explicit JSON instruction to prompt:
   ```rust
   let prompt = format!("{}\n\nIMPORTANT: Output ONLY valid JSON, no extra text.", prompt);
   ```
3. Implement retry logic with ToolGuard validation
4. Use MockLlm as fallback for critical paths

---

## üìà Performance Tuning

### Recommended Settings by Hardware

**Budget (GTX 1660, 6GB VRAM)**:
- Model: `phi3:mini`
- Concurrent requests: 2-3
- Max tokens: 128
- Expected: 40-60 tokens/sec

**Mid-range (RTX 3060, 12GB VRAM)** ‚≠ê **Recommended**:
- Model: `phi3:medium`
- Concurrent requests: 3-5
- Max tokens: 256-512
- Expected: 30-40 tokens/sec

**High-end (RTX 4090, 24GB VRAM)**:
- Model: `phi3:medium` or `phi3:large`
- Concurrent requests: 5-10
- Max tokens: 512-1024
- Expected: 50-80 tokens/sec (medium), 20-30 tokens/sec (large)

**CPU-only (16GB+ RAM)**:
- Model: `phi3:mini`
- Concurrent requests: 1-2
- Max tokens: 128
- Expected: 5-10 tokens/sec (very slow, not recommended for real-time)

### Concurrency Limits

```rust
// Conservative (low VRAM)
let scheduler = LlmScheduler::new(phi3, 2, 30);

// Balanced (recommended)
let scheduler = LlmScheduler::new(phi3, 5, 30);

// Aggressive (high-end GPU)
let scheduler = LlmScheduler::new(phi3, 10, 30);
```

**Rule of thumb**: `max_concurrent = (VRAM_GB / 2) - 1`

---

## üîê Security & Privacy

### Data Privacy

**Ollama runs 100% locally**:
- ‚úÖ No data sent to cloud/external servers
- ‚úÖ No API keys or accounts required
- ‚úÖ Game state never leaves your machine
- ‚úÖ Safe for competitive/unreleased games

### Resource Isolation

**Ollama uses system resources**:
- CPU/GPU: 30-70% utilization during inference
- RAM: ~8-10GB for phi3:medium
- Network: None (localhost only)

**Recommendation**: Monitor system resources, adjust `max_concurrent` if game performance degrades.

---

## üìö Next Steps

1. **Read `WEEK_4_ACTION_17_PHI3_COMPLETE.md`** - Full technical details
2. **Explore `astraweave-llm/src/prompts.rs`** - Customize prompt templates
3. **Run `examples/hello_companion`** - See Phi-3 in action
4. **Join Discord** - Share your Phi-3 experiences with the community

**Pro tip**: Start with `phi3:mini` for testing, upgrade to `phi3:medium` for production.

---

**Version**: 1.0.0  
**Last Updated**: October 10, 2025  
**Tested With**: Ollama 0.5.0+, Phi-3 Medium Q4
