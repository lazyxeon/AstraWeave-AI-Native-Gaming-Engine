# AstraWeave Performance Benchmarking Guide

This document describes the performance benchmarking infrastructure for the AstraWeave AI Native Gaming Engine.

## Overview

The AstraWeave benchmarking system provides automated performance tracking for core engine components using Criterion.rs and GitHub Actions integration. It's designed to:

- Track performance regressions automatically
- Provide detailed performance metrics for key operations
- Generate visual performance trends over time
- Alert on significant performance changes

## Benchmark Structure

### Benchmarked Components

1. **astraweave-core**: Core engine operations
   - `world_creation`: World initialization performance
   - `entity_spawning`: Entity creation and management
   - `world_tick`: Core game loop performance

2. **astraweave-input**: Input system performance  
   - `binding_creation`: Input binding creation
   - `binding_serialization`: Input binding serialization
   - `binding_deserialization`: Input binding deserialization
   - `binding_set_creation`: Input binding set operations

### Benchmark Files

- `astraweave-core/benches/core_benchmarks.rs`
- `astraweave-input/benches/input_benchmarks.rs`

## Workflow Integration

### Automated Execution

The benchmark workflow (`.github/workflows/benchmark.yml`) runs:

- On pushes to `main` and `develop` branches
- On pull requests to `main` and `develop` branches
- Can be triggered manually via workflow dispatch

### Performance Tracking

- **Main Branch**: Results are automatically stored and tracked over time
- **Pull Requests**: Performance comparison comments are added to PRs
- **Alerts**: Automatic alerts on regressions > 200% performance degradation

### Optimization Features

- **Advanced Caching**: Multi-level Rust compilation caching
- **sccache Integration**: Distributed compilation cache
- **Concurrency Control**: Prevents conflicting workflow runs
- **Timeout Protection**: 45-minute timeout prevents hanging jobs

## Understanding Results

### Output Format

Benchmarks generate results in multiple formats:

1. **JSON Data**: Machine-readable performance metrics
2. **Summary Report**: Human-readable performance summary
3. **Visual Graphs**: Generated by GitHub Action Benchmark

### Interpreting Metrics

Performance metrics are reported in nanoseconds (ns) and automatically converted to appropriate units:

- **< 1,000 ns**: Displayed in nanoseconds (ns)
- **< 1,000,000 ns**: Displayed in microseconds (µs)  
- **< 1,000,000,000 ns**: Displayed in milliseconds (ms)
- **≥ 1,000,000,000 ns**: Displayed in seconds (s)

### Expected Performance Ranges

> **Note:** The following performance ranges are preliminary estimates and may not reflect real-world results. Please validate against actual benchmark data for your environment.

| Benchmark | Expected Range | Notes |
|-----------|----------------|-------|
| world_creation | 50-100 ns | Preliminary estimate; should be very fast |
| entity_spawning | 15-30 µs | Preliminary estimate; per 100 entities |
| world_tick | 40-80 ns | Preliminary estimate; single tick, empty world |
| binding_creation | 2-5 ns | Preliminary estimate; simple structure creation |
| binding_serialization | 50-100 ns | Preliminary estimate; JSON serialization |
| binding_deserialization | 80-150 ns | Preliminary estimate; JSON parsing |
| binding_set_creation | 500-1000 ns | Preliminary estimate; complex structure |

## Local Development

### Running Benchmarks Locally

```bash
# Run all benchmarks
cargo bench

# Run specific package benchmarks
cargo bench -p astraweave-core
cargo bench -p astraweave-input

# Run specific benchmark
cargo bench -p astraweave-core world_creation
```

### Benchmark Development

When adding new benchmarks:

1. Create benchmark files in `<crate>/benches/`
2. Add `[[bench]]` entries to `Cargo.toml`
3. Use Criterion.rs for consistent measurement
4. Follow existing naming conventions

Example benchmark structure:

```rust
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use my_crate::MyStruct;

fn bench_my_operation(c: &mut Criterion) {
    c.bench_function("my_operation", |b| {
        b.iter(|| {
            // Setup
            let data = MyStruct::new();
            
            // Benchmark this operation
            black_box(data.my_operation());
        })
    });
}

criterion_group!(benches, bench_my_operation);
criterion_main!(benches);
```

### Requirements

Local benchmark execution requires:

- Rust 1.89.0+ (pinned in `rust-toolchain.toml`)
- System dependencies (Linux):
  ```bash
  sudo apt-get install -y build-essential pkg-config cmake ninja-build \
    libx11-dev libxi-dev libxcursor-dev libxrandr-dev libxinerama-dev \
    libxkbcommon-dev libxkbcommon-x11-dev libx11-xcb-dev libxcb1-dev \
    libxcb-randr0-dev libxcb-xfixes0-dev libxcb-shape0-dev libxcb-xkb-dev \
    libgl1-mesa-dev libegl1-mesa-dev wayland-protocols libwayland-dev \
    libasound2-dev libpulse-dev libudev-dev
  ```

## CI/CD Integration

### Workflow Configuration

Key environment variables:
- `RUSTC_WRAPPER: sccache` - Enables compilation caching
- `SCCACHE_CACHE_SIZE: "10G"` - Cache size limit
- `CARGO_NET_RETRY: 10` - Network retry attempts

### Caching Strategy

The workflow implements multi-level caching:

1. **Rust Cache**: Dependencies and compiled artifacts
2. **sccache**: Compilation unit caching
3. **Target Cache**: Build outputs

### Performance Monitoring

The system tracks:
- Compilation times across builds
- Cache hit rates
- Memory usage during benchmarks
- Performance trends over time

## Troubleshooting

### Common Issues

1. **Benchmarks fail to compile**
   - Check system dependencies are installed
   - Verify Rust toolchain version
   - Check for conflicting dependencies

2. **Performance inconsistency**
   - Ensure consistent test environment
   - Check for background processes
   - Consider hardware variations

3. **Timeout errors**
   - Benchmarks are limited to 45 minutes
   - Large workspace can cause timeouts
   - Check for infinite loops in benchmark code

### Debug Mode

Enable verbose output in workflows:
```yaml
env:
  VERBOSE: "true"
```

### Manual Script Execution

Test the benchmark runner locally:
```bash
export BENCHMARK_RESULTS_DIR="local_results"
export VERBOSE="true"
./.github/scripts/benchmark-runner.sh
```

## Best Practices

### Benchmark Design

1. **Use `black_box()`**: Prevent compiler optimizations
2. **Consistent Setup**: Minimize setup time in measurements
3. **Representative Data**: Use realistic input sizes
4. **Stable Environment**: Minimize external factors

### Performance Analysis

1. **Trend Analysis**: Focus on trends over absolute values
2. **Statistical Significance**: Consider measurement variance
3. **Context Awareness**: Account for system differences
4. **Regular Monitoring**: Review performance regularly

### Development Workflow

1. **Baseline First**: Establish performance baselines
2. **Incremental Testing**: Test changes incrementally  
3. **Performance Review**: Include performance in code reviews
4. **Documentation**: Document expected performance characteristics

## Future Improvements

Planned enhancements:
- [ ] Cross-platform benchmark comparison
- [ ] Memory usage benchmarking
- [ ] Integration with performance profiling tools
- [ ] Automated performance regression analysis
- [ ] Historical performance data visualization
- [ ] Benchmark result comparison tools