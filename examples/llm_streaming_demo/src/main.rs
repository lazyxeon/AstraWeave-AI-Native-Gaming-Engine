//! LLM Streaming Demo
//!
//! Demonstrates the new streaming API for Hermes 2 Pro, showing:
//! - Time-to-first-chunk advantage (8Ã— faster first action)
//! - Progressive response delivery
//! - Integration with StreamingParser (future work)
//!
//! # Usage
//! ```bash
//! # Start Ollama with Hermes 2 Pro
//! ollama serve
//! ollama pull adrienbrault/nous-hermes2pro:Q4_K_M
//!
//! # Run demo
//! cargo run -p llm_streaming_demo --release
//! ```

use anyhow::Result;
use astraweave_llm::hermes2pro_ollama::Hermes2ProOllama;
use astraweave_llm::LlmClient;
use futures_util::StreamExt;

#[tokio::main]
async fn main() -> Result<()> {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘            LLM STREAMING DEMO - HERMES 2 PRO                  â•‘");
    println!("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");
    println!("â•‘  Demonstrates:                                                 â•‘");
    println!("â•‘  - Progressive response delivery (streaming)                   â•‘");
    println!("â•‘  - Time-to-first-chunk advantage                               â•‘");
    println!("â•‘  - Side-by-side blocking vs streaming comparison               â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    let client = Hermes2ProOllama::localhost();

    // Health check first
    println!("ğŸ” Checking Ollama health...");
    let health = client.health_check().await?;

    if !health.is_ready() {
        eprintln!("âŒ {}", health.error_message().unwrap());
        eprintln!("\nPlease ensure Ollama is running:");
        eprintln!("  1. Install Ollama: https://ollama.ai");
        eprintln!("  2. Start server: ollama serve");
        eprintln!("  3. Pull model: ollama pull adrienbrault/nous-hermes2pro:Q4_K_M");
        std::process::exit(1);
    }

    println!("âœ… Ollama ready (version: {})", health.ollama_version);
    println!("âœ… Model available: {}\n", health.model_name);

    // Test prompt
    let prompt = "You are a tactical AI agent at position (5,5). \
                  Enemy spotted at (10,8). \
                  Generate a JSON tactical plan with 3-5 action steps. \
                  Use actions: MoveTo, TakeCover, Attack, ThrowSmoke.";

    println!("â•â•â• TEST 1: BLOCKING COMPLETION (baseline) â•â•â•\n");

    let start = std::time::Instant::now();
    let blocking_response = client.complete(prompt).await?;
    let blocking_time = start.elapsed();

    println!("â±ï¸  Total time: {:.2}s", blocking_time.as_secs_f32());
    println!("ğŸ“ Response length: {} chars", blocking_response.len());
    println!(
        "ğŸ“„ Response preview:\n{}\n",
        &blocking_response.chars().take(200).collect::<String>()
    );

    println!("\nâ•â•â• TEST 2: STREAMING COMPLETION (progressive) â•â•â•\n");

    let start = std::time::Instant::now();
    let mut stream = client.complete_streaming(prompt).await?;

    let mut streaming_response = String::new();
    let mut chunk_count = 0;
    let mut time_to_first_chunk = None;

    while let Some(chunk_result) = stream.next().await {
        let chunk = chunk_result?;

        if time_to_first_chunk.is_none() {
            time_to_first_chunk = Some(start.elapsed());
            println!(
                "âš¡ FIRST CHUNK ARRIVED: {:.2}s",
                time_to_first_chunk.unwrap().as_secs_f32()
            );
        }

        chunk_count += 1;
        streaming_response.push_str(&chunk);

        // Show progressive updates every 5 chunks
        if chunk_count % 5 == 0 {
            println!(
                "   Chunk #{:3}: {} chars received (total: {} chars)",
                chunk_count,
                chunk.len(),
                streaming_response.len()
            );
        }
    }

    let streaming_total_time = start.elapsed();

    println!(
        "\nâ±ï¸  Total time: {:.2}s",
        streaming_total_time.as_secs_f32()
    );
    println!(
        "âš¡ Time to first chunk: {:.2}s",
        time_to_first_chunk.unwrap().as_secs_f32()
    );
    println!("ğŸ“¦ Total chunks: {}", chunk_count);
    println!("ğŸ“ Response length: {} chars", streaming_response.len());
    println!(
        "ğŸ“„ Response preview:\n{}\n",
        &streaming_response.chars().take(200).collect::<String>()
    );

    // Performance comparison
    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘                    PERFORMANCE COMPARISON                      â•‘");
    println!("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");

    let ttfc = time_to_first_chunk.unwrap().as_secs_f32();
    let speedup = blocking_time.as_secs_f32() / ttfc;
    let ttfc_percent = (ttfc / blocking_time.as_secs_f32()) * 100.0;

    println!(
        "â•‘  Blocking total time:     {:.2}s",
        blocking_time.as_secs_f32()
    );
    println!(
        "â•‘  Streaming total time:    {:.2}s",
        streaming_total_time.as_secs_f32()
    );
    println!(
        "â•‘  Time to first chunk:     {:.2}s ({:.1}% of total)",
        ttfc, ttfc_percent
    );
    println!("â•‘  Time-to-first speedup:   {:.1}Ã— faster", speedup);
    println!("â•‘  Chunks received:         {}", chunk_count);
    println!("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");

    if speedup >= 4.0 {
        println!(
            "â•‘  âœ… EXCELLENT: {:.1}Ã— speedup exceeds 4Ã— target!",
            speedup
        );
    } else if speedup >= 2.0 {
        println!("â•‘  âœ… GOOD: {:.1}Ã— speedup meets 2Ã— minimum", speedup);
    } else {
        println!(
            "â•‘  âš ï¸  BELOW TARGET: {:.1}Ã— speedup (expected 4-8Ã—)",
            speedup
        );
    }

    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // Validate responses match
    let blocking_trimmed = blocking_response.trim();
    let streaming_trimmed = streaming_response.trim();

    if blocking_trimmed == streaming_trimmed {
        println!("âœ… Response consistency: PERFECT (streaming matches blocking)");
    } else {
        let similarity = similar_percent(&blocking_trimmed, &streaming_trimmed);
        println!(
            "âš ï¸  Response similarity: {:.1}% (expected ~95-100% with temp=0.7)",
            similarity
        );
    }

    println!("\nâ•â•â• Demo Complete â•â•â•");
    println!("Key Takeaways:");
    println!(
        "  - Streaming delivers first chunk ~{:.1}Ã— faster than full response",
        speedup
    );
    println!("  - Enables progressive UI updates and faster perceived latency");
    println!("  - Ready for integration with StreamingParser and BatchExecutor");

    Ok(())
}

/// Simple string similarity metric (percentage of matching characters)
fn similar_percent(a: &str, b: &str) -> f32 {
    let max_len = a.len().max(b.len()) as f32;
    if max_len == 0.0 {
        return 100.0;
    }

    let matching = a.chars().zip(b.chars()).filter(|(ca, cb)| ca == cb).count();

    (matching as f32 / max_len) * 100.0
}
