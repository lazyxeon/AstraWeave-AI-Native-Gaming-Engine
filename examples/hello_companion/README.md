# hello_companion - AI Companion Demo

**Showcase of AstraWeave's hybrid AI architecture: Classical AI + LLM with graceful fallback**

## Overview

This example demonstrates:
- **Classical AI**: GOAP-based planning (always works, no dependencies)
- **LLM AI**: Phi-3 language model planning (optional, requires model download)
- **Hybrid Mode**: Try LLM first, fallback to classical on errors
- **Comparison Demo**: Side-by-side evaluation of both systems

## Quick Start

### Classical AI (Default)
```bash
# No dependencies, works immediately
cargo run -p hello_companion --release

# Expected output:
# ðŸ’¡ LLM feature not enabled. Using classical AI.
# ðŸ¤– Using Classical AI (RuleOrchestrator)...
#    Classical plan: 3 steps (0.023ms)
```

### LLM AI (Optional)
```bash
# Enable LLM feature (uses MockLlm for demo)
cargo run -p hello_companion --release --features llm

# Expected output:
# âœ… LLM feature enabled. Using hybrid AI (LLM + fallback).
# ðŸŽ¯ Trying LLM AI with classical fallback...
#    LLM returned fallback plan: [reason]
```

### Comparison Demo
```bash
# Compare both systems side-by-side
cargo run -p hello_companion --release --features llm -- --demo-both

# Expected output:
# --- CLASSICAL AI ---
#    Classical plan: 3 steps (0.014ms)
#
# --- LLM AI (MockLlm) ---
#    LLM returned fallback plan: [reason]
#
# --- COMPARISON ---
# Classical steps: 3
# LLM steps:       0
```

## AI Modes

### Classical AI
- **Type**: Rule-based GOAP planner (`RuleOrchestrator`)
- **Latency**: ~0.02ms (sub-millisecond)
- **Reliability**: 100% (deterministic, no external dependencies)
- **Use Case**: Production gameplay (fast, reliable, always works)

### LLM AI
- **Type**: Phi-3 language model (via MockLlm demo or real Phi-3)
- **Latency**: 50-2000ms (model inference time)
- **Reliability**: 85-95% (depends on model, prompt, validation)
- **Fallback**: Automatic fallback to classical AI on errors
- **Use Case**: Complex reasoning, narrative generation, creative behaviors

### Hybrid Mode (Recommended)
- **Strategy**: Try LLM â†’ Fallback to classical
- **Latency**: LLM latency on success, classical latency on fallback
- **Reliability**: 100% (classical AI guarantees valid plan)
- **Use Case**: Best of both worlds (LLM intelligence + classical reliability)

### ðŸ¤– Arbiter Mode (NEW - Zero-Latency Hybrid)
- **Type**: GOAP+Hermes Hybrid Arbiter (`AIArbiter`)
- **Latency**: **101.7 ns** GOAP control (instant tactical decisions)
- **Intelligence**: Hermes 2 Pro LLM plans generate asynchronously (13-21s background)
- **Reliability**: 100% (GOAP provides instant responses while LLM thinks)
- **Use Case**: Production-ready AI with zero user-facing latency

**Key Innovation**: Agents respond instantly with GOAP tactical AI while LLM plans generate in the background. When LLM completes, agents smoothly transition to executing strategic plans, then return to GOAP for the next challenge.

**Try it now**:
```bash
# Run with arbiter (zero-latency hybrid control)
cargo run -p hello_companion --release --features llm_orchestrator -- --arbiter

# Expected output (instant GOAP actions, no 13-21s delays):
# Frame 1: [GOAP] MoveTo(5, 5)
# Frame 2: [GOAP] MoveTo(6, 5)
# Frame 3: [ExecutingLLM(0)] TakeCover(7, 8)  # LLM plan ready, executing step 0
# Frame 4: [ExecutingLLM(1)] MoveTo(8, 9)     # Executing step 1
# Frame 5: [ExecutingLLM(2)] Attack(enemy_1)  # Executing step 2
# Frame 6: [GOAP] MoveTo(10, 10)              # Back to GOAP, ready for next LLM
```

**Performance Highlights**:
- **GOAP Control**: 101.7 ns per update (982Ã— faster than 100 Âµs target)
- **LLM Polling**: 575.3 ns per update (checking background task status)
- **Mode Transitions**: 221.9 ns (GOAP â†” ExecutingLLM seamless)
- **Scalability**: 1,000 agents @ 60 FPS = 0.6% frame budget, 10,000 agents = 6.1%

ðŸ“š **Documentation**:
- [Complete Implementation Guide (8,000 words)](../../docs/ARBITER_IMPLEMENTATION.md) - Architecture, performance analysis, integration guide
- [Quick Reference (5 min read)](../../docs/ARBITER_QUICK_REFERENCE.md) - API docs, common patterns, troubleshooting

## Features

### Feature Flags
```toml
# Cargo.toml
[features]
default = []
llm = ["astraweave-llm", "astraweave-llm/llm_cache", "tokio"]
```

- **default**: Classical AI only (no dependencies, fast build)
- **llm**: Enables LLM integration (adds ~30s to first build)

### Command-Line Arguments
- `--demo-both`: Run side-by-side comparison of classical vs LLM

## Implementation Details

### Code Structure
```rust
// AI mode selection
enum AIMode {
    Classical,             // RuleOrchestrator (default)
    #[cfg(feature = "llm")]
    LLM,                  // MockLlm or Phi-3
    #[cfg(feature = "llm")]
    Hybrid,               // Try LLM, fallback to classical
}

// Plan generation
fn generate_plan(snap: &WorldSnapshot, mode: AIMode) -> Result<PlanIntent> {
    match mode {
        AIMode::Classical => generate_classical_plan(snap),
        AIMode::LLM => generate_llm_plan(snap).await,
        AIMode::Hybrid => /* try LLM, fallback to classical */
    }
}
```

### LLM Integration
```rust
#[cfg(feature = "llm")]
fn generate_llm_plan(snap: &WorldSnapshot) -> Result<PlanIntent> {
    use astraweave_llm::{plan_from_llm, MockLlm};
    
    let client = MockLlm;  // Or OllamaClient for real Phi-3
    let registry = create_tool_registry();
    
    // Call async LLM API
    let rt = tokio::runtime::Runtime::new()?;
    let result = rt.block_on(async {
        plan_from_llm(&client, snap, &registry).await
    });
    
    match result {
        PlanSource::Llm(plan) => Ok(plan),      // LLM succeeded
        PlanSource::Fallback { plan, reason } => Ok(plan),  // LLM failed, heuristic plan
    }
}
```

### Tool Registry
The LLM has access to these tools:
- **MoveTo(x, y)**: Navigate to grid position
- **Throw(item, x, y)**: Throw smoke/grenade to position
- **CoverFire(target_id, duration)**: Suppress enemy with covering fire

## Performance

| Mode      | Latency   | Reliability | Use Case                |
|-----------|-----------|-------------|-------------------------|
| Classical | ~0.02ms   | 100%        | Production gameplay     |
| LLM       | 50-2000ms | 85-95%      | Complex reasoning       |
| Hybrid    | Variable  | 100%        | Best of both worlds     |

## Extending

### Add Real Phi-3 Model
```rust
// Replace MockLlm with OllamaClient
#[cfg(feature = "ollama")]
use astraweave_llm::OllamaClient;

let client = OllamaClient {
    url: "http://localhost:11434".to_string(),
    model: "phi3:mini".to_string(),
};
```

### Add Custom Tools
```rust
fn create_tool_registry() -> ToolRegistry {
    ToolRegistry {
        tools: vec![
            // Add your custom tool
            ToolSpec {
                name: "CustomAction".into(),
                args: {
                    let mut m = BTreeMap::new();
                    m.insert("param1".into(), "i32".into());
                    m
                },
            },
        ],
        // ...
    }
}
```

### Add Custom AI Mode
```rust
enum AIMode {
    Classical,
    LLM,
    Hybrid,
    Custom,  // Your custom AI
}

fn generate_plan(snap: &WorldSnapshot, mode: AIMode) -> Result<PlanIntent> {
    match mode {
        // ...
        AIMode::Custom => generate_custom_plan(snap),
    }
}
```

## Troubleshooting

### "LLM feature not enabled"
**Solution**: Add `--features llm` to cargo command
```bash
cargo run -p hello_companion --release --features llm
```

### "LLM returned fallback plan"
**Cause**: MockLlm intentionally triggers fallback for demo purposes

**Solution**: Use real Phi-3 model with OllamaClient:
1. Install Ollama: https://ollama.ai
2. Download Phi-3: `ollama pull phi3:mini`
3. Enable ollama feature: `cargo run -p hello_companion --features llm,ollama`

### Long first build with LLM
**Cause**: tokio + astraweave-llm dependencies (~30s)

**Solution**: Build without LLM for faster iteration:
```bash
cargo run -p hello_companion --release  # Classical only, fast build
```

## Related Examples

- **core_loop_goap_demo**: Pure GOAP planning (classical AI)
- **core_loop_bt_demo**: Behavior tree AI (classical AI)
- **unified_showcase**: Full game demo with AI companions

## See Also

- **COMPREHENSIVE_STRATEGIC_ANALYSIS.md**: AstraWeave LLM architecture
- **AI_NATIVE_VALIDATION_REPORT.md**: LLM performance benchmarks (81 tests)
- **astraweave-llm/README.md**: LLM crate documentation
- **astraweave-ai/README.md**: Classical AI documentation
